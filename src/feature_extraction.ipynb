{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **REVERSE IMAGE SEARCH ENGINE - FEATURE EXTRACTION**"
      ],
      "metadata": {
        "id": "VHBZFuC3nUH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTS"
      ],
      "metadata": {
        "id": "K3B2kpOlnEla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "BFdr0vRzg0Mg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILITIES"
      ],
      "metadata": {
        "id": "c6odDMy-nK-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(img_path, feature_model):\n",
        "    img = load_img(img_path, target_size=(img_width, img_height))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    features = feature_model.predict(img_array)\n",
        "    flattened = features.flatten()\n",
        "    normalized = flattened / norm(flattened)\n",
        "    return normalized"
      ],
      "metadata": {
        "id": "PX9F96Tbg-VX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
        "\n",
        "def get_file_paths(root_dir):\n",
        "    file_list = []\n",
        "    for root, directories, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if any(ext in filename for ext in extensions):\n",
        "                filepath = os.path.join(root, filename)\n",
        "                if os.path.exists(filepath):\n",
        "                  file_list.append(filepath)\n",
        "                else:\n",
        "                  print(filepath)\n",
        "    return file_list"
      ],
      "metadata": {
        "id": "LpvUttO0tYrA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model():\n",
        "    base_model = ResNet50(include_top=False, input_shape=(img_width, img_height, 3), pooling='avg')\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    input_tensor = Input(shape=(img_width, img_height, 3))\n",
        "    x = base_model(input_tensor)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    output_tensor = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    full_model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "    return full_model, base_model"
      ],
      "metadata": {
        "id": "3L1DDun_kZYv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA DOWNLOAD"
      ],
      "metadata": {
        "id": "95nHreI6g1Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/caltech101', exist_ok=True)\n",
        "\n",
        "!curl -L \"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip?download=1\" -o \"/content/caltech101/caltech-101.zip\"\n",
        "!unzip \"/content/caltech101/caltech-101.zip\" -d \"/content/caltech101/\"\n",
        "!tar -xzf /content/caltech101/caltech-101/101_ObjectCategories.tar.gz -C /content/caltech101/caltech-101/\n",
        "!rm -rf /content/101_ObjectCategories/BACKGROUND_Google\n",
        "!rm -rf /content/caltech101/__MACOSX\n",
        "!rm -rf /content/caltech101/caltech-101.zip\n",
        "!rm -rf /content/caltech101/caltech-101/101_ObjectCategories/BACKGROUND_Google"
      ],
      "metadata": {
        "id": "OEtRHiPtfJ4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebde7bc-32cc-4883-bb66-8f2f8136e180"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   476  100   476    0     0    795      0 --:--:-- --:--:-- --:--:--   794\n",
            "100  131M  100  131M    0     0  8688k      0  0:00:15  0:00:15 --:--:-- 8355k\n",
            "Archive:  /content/caltech101/caltech-101.zip\n",
            "   creating: /content/caltech101/caltech-101/\n",
            "  inflating: /content/caltech101/__MACOSX/._caltech-101  \n",
            "  inflating: /content/caltech101/caltech-101/101_ObjectCategories.tar.gz  \n",
            "  inflating: /content/caltech101/__MACOSX/caltech-101/._101_ObjectCategories.tar.gz  \n",
            "  inflating: /content/caltech101/caltech-101/show_annotation.m  \n",
            "  inflating: /content/caltech101/__MACOSX/caltech-101/._show_annotation.m  \n",
            "  inflating: /content/caltech101/caltech-101/Annotations.tar  \n",
            "  inflating: /content/caltech101/__MACOSX/caltech-101/._Annotations.tar  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE EXTRACTION"
      ],
      "metadata": {
        "id": "t7AnrNCftP--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/caltech101/caltech-101/101_ObjectCategories'\n",
        "\n",
        "filenames = sorted(get_file_paths(root_dir))\n",
        "print(f'There are {len(filenames)} files in the dataset.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuUlQAhfh4jA",
        "outputId": "0591e618-0dc4-4fcb-96c5-a9e767923ed2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 8677 files in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = 8677\n",
        "num_classes = 101\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "s57sX1Lkkc7m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   rotation_range=25,\n",
        "                                   width_shift_range=0.15,\n",
        "                                   height_shift_range=0.15,\n",
        "                                   zoom_range=0.3)"
      ],
      "metadata": {
        "id": "4KeGRpUtkfpC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(root_dir,\n",
        "                                                    target_size=(img_width, img_height),\n",
        "                                                    shuffle=True,\n",
        "                                                    seed=10000,\n",
        "                                                    class_mode='categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAPz6gsYkiV1",
        "outputId": "61db79e6-e10a-4141-9d2e-6364638126f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8677 images belonging to 101 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = len(train_generator.filenames)\n",
        "steps_per_epochs = int(math.ceil(num_images / batch_size))\n",
        "print(f'Number of images: {num_images}')\n",
        "print(f'Number of steps per epochs: {steps_per_epochs}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY0Djy8lEWl2",
        "outputId": "89aec193-a083-446e-8289-9d45b2c1f3e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images: 8677\n",
            "Number of steps per epochs: 68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [root_dir + '/' + s for s in train_generator.filenames]"
      ],
      "metadata": {
        "id": "NG3_VqU6j--u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_finetuned, feature_extractor = define_model()\n",
        "model_finetuned.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(0.001),\n",
        "    metrics=['acc']\n",
        ")\n",
        "\n",
        "model_finetuned.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epochs,\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXp2bJRmknNr",
        "outputId": "29d496b9-54cd-4d7d-bcbf-ff85e481b689"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 399ms/step - acc: 0.2058 - loss: 4.0095\n",
            "Epoch 2/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 458ms/step - acc: 0.4136 - loss: 2.6702\n",
            "Epoch 3/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 415ms/step - acc: 0.5276 - loss: 2.0453\n",
            "Epoch 4/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 404ms/step - acc: 0.5785 - loss: 1.7031\n",
            "Epoch 5/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 408ms/step - acc: 0.6426 - loss: 1.4548\n",
            "Epoch 6/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 398ms/step - acc: 0.6665 - loss: 1.2592\n",
            "Epoch 7/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 401ms/step - acc: 0.6950 - loss: 1.1590\n",
            "Epoch 8/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 409ms/step - acc: 0.7066 - loss: 1.1007\n",
            "Epoch 9/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 392ms/step - acc: 0.7330 - loss: 0.9998\n",
            "Epoch 10/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 392ms/step - acc: 0.7383 - loss: 0.9346\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e9bf566c250>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_features = np.array([extract_features(img_path, feature_extractor) for img_path in filenames])\n",
        "np.save(\"features.npy\", all_features)\n",
        "np.save(\"filenames.npy\", filenames)"
      ],
      "metadata": {
        "id": "RWxIZrk6fVnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_ids = train_generator.classes\n",
        "np.save(\"class_ids.npy\", class_ids)"
      ],
      "metadata": {
        "id": "ZjeF5MoICZLz"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}